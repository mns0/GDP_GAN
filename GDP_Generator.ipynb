{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GDP_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKWWlo1TveS1YrACmULQS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mns0/GDP_GAN/blob/master/GDP_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NonJmZvHWA6w",
        "colab_type": "code",
        "outputId": "f74d9b88-a8ca-4468-f165-4a9a14d9e3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from torchsummary import summary\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPf-v7bjhlJO",
        "colab_type": "code",
        "outputId": "5692e129-bfa8-492e-9849-b10d7bd67f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Mar 21 19:35:12 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw2Tlyr5WJ76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import init\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeWiN7oTWtfo",
        "colab_type": "code",
        "outputId": "608f7aab-2eca-468a-a3ee-06f508f60ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "#get the data\n",
        "dir = './drive/My Drive/gdp_data/'\n",
        "#preprocessed in a previous \n",
        "train_data = 'Preprocessed_data_gdp_pc.dat'\n",
        "gdp = pd.read_csv(dir+train_data)\n",
        "#m = (gdp.columns != 'GDP') & (gdp.columns != 'Date') \n",
        "gdp = gdp.drop(['Unnamed: 0'], axis = 1).astype('float')\n",
        "#gdp.loc[:,m]\n",
        "#make even column dataframe\n",
        "#gdp.drop(gdp[(gdp.shape[0]//15 * 15):].index, inplace=True)\n",
        "#df_split = np.split(gdp.to_numpy(),gdp.shape[0]//15)\n",
        "#df_split = np.asarray(df_split,dtype=\"float\")\n",
        "#df_split[0][0]\n",
        "print(gdp.head())\n",
        "\n",
        "for idx, c in enumerate(gdp.columns):\n",
        "  l,m = gdp[c].min(), gdp[c].max()\n",
        "  gdp[c] = 2*((gdp[c] - l)/(m-l))  - 1\n",
        "\n",
        "\n",
        "print(gdp.head())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   CPILFESL  DFF  DTB3  DGS5  ...  PSAVERT   DSPI  GFDEGDQ188S      GDP\n",
            "0      39.6  9.5  7.80  8.08  ...     11.8  732.1     30.08057  1.25734\n",
            "1      39.6  9.5  7.80  8.08  ...     11.8  732.1     30.08057  1.25734\n",
            "2      39.6  9.5  7.79  8.03  ...     11.8  732.1     30.08057  1.25734\n",
            "3      39.6  9.0  7.82  8.08  ...     11.8  732.1     30.08057  1.25734\n",
            "4      39.6  8.5  7.83  8.06  ...     11.8  732.1     30.08057  1.25734\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "   CPILFESL       DFF      DTB3  ...  DSPI  GFDEGDQ188S       GDP\n",
            "0      -1.0 -0.152330 -0.088578  ...  -1.0    -0.871205 -0.191709\n",
            "1      -1.0 -0.152330 -0.088578  ...  -1.0    -0.871205 -0.191709\n",
            "2      -1.0 -0.152330 -0.089744  ...  -1.0    -0.871205 -0.191709\n",
            "3      -1.0 -0.197133 -0.086247  ...  -1.0    -0.871205 -0.191709\n",
            "4      -1.0 -0.241935 -0.085082  ...  -1.0    -0.871205 -0.191709\n",
            "\n",
            "[5 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mzJ4pjBX3qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a dataloader\n",
        "class GDPData(Dataset):\n",
        "  \"\"\"GDP time series \"\"\"\n",
        "  def __init__(self, csv_file, series_length = 15, normalize = False):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "    if normalize:\n",
        "      self.normalize_table = np.zeros((df.shape[1],2)) #(min, max)\n",
        "      self.normalize(df)\n",
        "\n",
        "    df.drop(df[(df.shape[0]//series_length * series_length):].index, inplace=True)\n",
        "    x = df.loc[:, (df.columns != 'GDP') ]\n",
        "\n",
        "    full = np.array(np.array_split(df.to_numpy(),df.shape[0]//series_length))\n",
        "    x = np.array(np.array_split(x.to_numpy(),x.shape[0]//series_length))\n",
        "    y = df[\"GDP\"]\n",
        "    y = np.array(np.array_split(y.to_numpy(),y.shape[0]//series_length))\n",
        "    self.t = x.shape[0]\n",
        "    #print(\"d\", type(x),x.shape.shape)\n",
        "    x = np.expand_dims(x,-1)\n",
        "    full = np.expand_dims(full,-1)\n",
        "    self.x = torch.from_numpy(x).float()\n",
        "    self.y = torch.from_numpy(y).float()\n",
        "    self.full = torch.from_numpy(full).float()\n",
        "\n",
        "\n",
        "  def normalize(self,x):\n",
        "    \"\"\"\n",
        "      Normalize data to [-1,1] range \n",
        "      for all predictors in dataframe\n",
        "    \"\"\"\n",
        "    if not hasattr(self,'normalize_table'):\n",
        "      raise Exception(\"Incorrectly calling normalize\")\n",
        "    for idx, c in enumerate(x.columns):\n",
        "      l,m = x[c].min(), x[c].max()\n",
        "      self.normalize_table[idx][0] = l\n",
        "      self.normalize_table[idx][1] = m\n",
        "      x[c] = 2*((x[c] - l)/(m-l))  - 1\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.t\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    xr  = self.x[idx]\n",
        "    yr  = self.y[idx]\n",
        "    #the conditional \n",
        "    full = self.full[idx]\n",
        "    return xr, yr, full\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQER95YH8awC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class StatefulLSTM(nn.Module):\n",
        "  def __init__(self,in_size,out_size):\n",
        "    super(StatefulLSTM, self).__init__()\n",
        "    self.lstm = nn.LSTMCell(in_size,out_size)\n",
        "    self.out_size = out_size\n",
        "    self.h, self.c = None, None\n",
        "  \n",
        "  def reset_state(self):\n",
        "    self.h, self.c = None, None\n",
        "  \n",
        "  def forward(self,x):\n",
        "    batch_size = x.data.size()[0]\n",
        "    if self.h is None:\n",
        "      state_size = [batch_size, self.out_size]\n",
        "      self.c = Variable(torch.zeros(state_size)).cuda()\n",
        "      self.h = Variable(torch.zeros(state_size)).cuda()\n",
        "    self.h, self.c = self.lstm(x,(self.h,self.c))\n",
        "    return self.h\n",
        "    \n",
        "class LockedDropout(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LockedDropout,self).__init__()\n",
        "    self.m = None\n",
        "  \n",
        "  def reset_state(self):\n",
        "    self.m = None\n",
        "  \n",
        "  def forward(self, x, dropout=0.5, train=True):\n",
        "    if train==False:\n",
        "      return x\n",
        "    \n",
        "    if (self.m is None):\n",
        "      self.m = x.data.new(x.size()).bernoulli_(1 - dropout)\n",
        "    mask = Variable(self.m, requires_grad=False) / (1 - dropout)\n",
        "    return mask * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZB0GPfXjYdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Generator_RNN_test(nn.Module):\n",
        "#   def __init__(self, predictor_dim, target_size=1, hidden_dim=1, num_layers=1, dropout_prob=0,train=True):\n",
        "#     super(Generator_RNN_test, self).__init__()\n",
        "#     self.train = train\n",
        "#     self.dropout_prob = dropout_prob\n",
        "#     self.lstm1 = StatefulLSTM(predictor_dim, hidden_dim)\n",
        "#     self.bn_lstm1= nn.BatchNorm1d(hidden_dim)\n",
        "#     self.dropout1 = LockedDropout() \n",
        "\n",
        "#     self.lstm2 = StatefulLSTM(hidden_dim, hidden_dim)\n",
        "#     self.bn_lstm2= nn.BatchNorm1d(hidden_dim)\n",
        "#     self.dropout2 = LockedDropout() \n",
        "\n",
        "\n",
        "#     self.fc_output = nn.Sequential(nn.Linear(hidden_dim*batch_size, target_size), nn.Tanh())\n",
        "\n",
        "#   def reset_state(self):\n",
        "#     self.lstm1.reset_state()\n",
        "#     self.dropout1.reset_state()\n",
        "\n",
        "#   def forward(self, input):\n",
        "#     #self.reset_state()\n",
        "#     # input - batch_size x time_steps x features\n",
        "#     #self.reset_state()\n",
        "#     batch_size, no_of_timesteps, features = input.size(0), input.size(1), input.size(2)\n",
        "#     outputs = []\n",
        "\n",
        "#     #for each timestep reduce the output\n",
        "#     for i in range(no_of_timesteps):\n",
        "#       #batch_size x features\n",
        "#       h = self.lstm1(input[:,i,:])\n",
        "#       h = self.bn_lstm1(h)\n",
        "#       h = self.dropout1(h, dropout=self.dropout_prob ,train=self.train)\n",
        "#       #batch_size x hidden_dim\n",
        "#       h = self.lstm2(h)\n",
        "#       h = self.bn_lstm2(h)\n",
        "#       h = self.dropout2(h, dropout=self.dropout_prob ,train=self.train)\n",
        "#       #batch_size x hidden_dim\n",
        "#       h = self.fc_output(h)\n",
        "#       #batch_size\n",
        "#       print(h.size())\n",
        "\n",
        "#       outputs.append(h)\n",
        "\n",
        "#     outputs = torch.stack(outputs) # time_steps, batch_size, hidden_dim \n",
        "#     print(\"c\", outputs.shape)\n",
        "#     #outputs = torch.squeeze(output,dim=2)\n",
        "#     #outputs = outputs.permute(1,2,0) # time_steps, features, batch_size\n",
        "\n",
        "#     pool = nn.MaxPool1d(no_of_timesteps)\n",
        "\n",
        "#     h = pool(outputs)\n",
        "#     h = h.view(h.size(0),-1)\n",
        "#     h = self.fc_output(h)\n",
        "#     return h\n",
        "\n",
        "\n",
        "\n",
        "class Generator_RNN(nn.Module):\n",
        "  def __init__(self, predictor_dim, seq_len=15, target_size=1, hidden_dim=1, num_layers=1, dropout_prob=0,train=True):\n",
        "    super(Generator_RNN, self).__init__()\n",
        "    self.train = train\n",
        "    self.dropout_prob = dropout_prob\n",
        "\n",
        "    self.lstm1 = StatefulLSTM(predictor_dim, hidden_dim)\n",
        "    self.bn_lstm1= nn.BatchNorm1d(hidden_dim)\n",
        "    self.dropout1 = LockedDropout() \n",
        "\n",
        "    self.lstm2 = StatefulLSTM(hidden_dim, hidden_dim)\n",
        "    self.bn_lstm2= nn.BatchNorm1d(hidden_dim)\n",
        "    self.dropout2 = LockedDropout() \n",
        "    self.fc_output = nn.Sequential(nn.Linear(hidden_dim, target_size), nn.Tanh())\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.lstm1.reset_state()\n",
        "    self.dropout1.reset_state()\n",
        "\n",
        "  def forward(self, inputx, inputc):\n",
        "    #self.reset_state()\n",
        "    # input - batch_size x time_steps x features\n",
        "    #self.reset_state()\n",
        "    input = torch.cat((inputx,inputc),2)\n",
        "    batch_size, no_of_timesteps, features = input.size(0), input.size(1), input.size(2)\n",
        "    outputs = []\n",
        "    #print(\"In generator input size\", input.shape)\n",
        "\n",
        "    #lstm on each sequence \n",
        "    for i in range(batch_size):\n",
        "      h = self.lstm1(input[i,:,:])\n",
        "      h = self.bn_lstm1(h)\n",
        "      h = self.dropout1(h)\n",
        "\n",
        "      h = self.lstm2(h)\n",
        "      h = self.bn_lstm2(h)\n",
        "      h = self.dropout2(h)\n",
        "      h = self.fc_output(h)\n",
        "      h = torch.squeeze(h)\n",
        "      outputs.append(h)    \n",
        "    \n",
        "    outputs = torch.stack(outputs) # batch_size, timesteps \n",
        "    return outputs\n",
        "\n",
        "\n",
        "class Discriminator_RNN(nn.Module):\n",
        "  def __init__(self, predictor_dim, target_size=1, hidden_dim=200, num_layers=1, dropout_prob=0,train=True):\n",
        "    super(Discriminator_RNN, self).__init__()\n",
        "    self.train = train\n",
        "    self.dropout_prob = dropout_prob\n",
        "    self.lstm1 = StatefulLSTM(predictor_dim, hidden_dim)\n",
        "    self.bn_lstm1= nn.BatchNorm1d(hidden_dim)\n",
        "    self.dropout1 = LockedDropout() \n",
        "    self.fc_output = nn.Sequential(nn.Linear(hidden_dim, target_size), nn.Sigmoid())\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.lstm1.reset_state()\n",
        "    self.dropout1.reset_state()\n",
        "\n",
        "  def forward(self, _input):\n",
        "    #self.reset_state()\n",
        "    # input - batch_size x time_steps x features\n",
        "    self.reset_state()\n",
        "    no_of_timesteps = _input.shape[1]\n",
        "    outputs = []\n",
        "    for i in range(no_of_timesteps):\n",
        "      h = self.lstm1(_input[:,i,:])\n",
        "      h = self.bn_lstm1(h)\n",
        "      h = self.dropout1(h,dropout=self.dropout_prob ,train=self.train)\n",
        "      outputs.append(h)\n",
        "\n",
        "    outputs = torch.stack(outputs) # time_steps, batch_size, features\n",
        "    outputs = outputs.permute(1,2,0) # time_steps, features, batch_size\n",
        "\n",
        "    pool = nn.MaxPool1d(no_of_timesteps)\n",
        "\n",
        "    h = pool(outputs)\n",
        "    h = h.view(h.size(0),-1)\n",
        "    h = self.fc_output(h)\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS6ylQstX58i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 20\n",
        "epochs = 200\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "gdp_dataset = GDPData(dir+train_data, normalize = True)\n",
        "dataloader = torch.utils.data.DataLoader(gdp_dataset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n",
        "\n",
        "kwagsD = {\"target_size\" : 1, \"predictor_dim\" : 10, \"hidden_dim\" : 200,\\\n",
        "         \"num_layers\" : 1, \"dropout_prob\" : 0, 'train' : True}\n",
        "\n",
        "kwagsG = {\"target_size\" : 1, \"predictor_dim\" : 10, \"hidden_dim\" : 200,\\\n",
        "         \"num_layers\" : 1, \"dropout_prob\" : 0, 'train' : True}\n",
        "\n",
        "\n",
        "netD = Discriminator_RNN(**kwagsD).to(device)\n",
        "netG = Generator_RNN(**kwagsG).to(device)\n",
        "\n",
        "criterion = nn.BCELoss().to(device)\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.001, \n",
        "                          betas=(0.9, 0.999), eps=1e-08, \n",
        "                          weight_decay=0, amsgrad=True)\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.001, \n",
        "                          betas=(0.9, 0.999), eps=1e-08, \n",
        "                          weight_decay=0, amsgrad=True)\n",
        "\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "#fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "#summary(netD,(20, 15, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2MKZDsu5MOo",
        "colab_type": "code",
        "outputId": "73e1ed84-031d-4d42-a421-47cfa0f48845",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  print(\"epoch: \", epoch)\n",
        "  for i, data in enumerate(dataloader):\n",
        "    #augment GDP data (x) with\n",
        "    #other monatery data (conditional)\n",
        "    #x + cond  (full)\n",
        "\n",
        "    ############################\n",
        "    # (1) Update D network: maximize log(D(x|y)) + log(1 - D(G(z|y)))\n",
        "    ###########################\n",
        "\n",
        "\n",
        "    conditional, x, full = data\n",
        "    conditional, x = conditional.to(device), x.to(device)\n",
        "    full = full.to(device)\n",
        "\n",
        "    if full.size(0) < 20:\n",
        "      continue\n",
        "\n",
        "\n",
        "    batch_size, seq_len = full.size(0), full.size(1)\n",
        "    netD.zero_grad()\n",
        "    label = torch.full((batch_size,), real_label, device=device)\n",
        "    output = netD(torch.squeeze(full,3)).view(-1)\n",
        "    errD_real = criterion(output, label)\n",
        "    errD_real.backward()\n",
        "    D_x = output.mean().item()\n",
        "\n",
        "\n",
        "    #training with the fake batch\n",
        "    #G(Z|Y) normal noise + condition\n",
        "    noise = torch.randn(batch_size, seq_len, 1, device=device)\n",
        "    conditional = torch.squeeze(conditional,3)\n",
        "\n",
        "    fake = netG(noise,conditional)\n",
        "    label.fill_(fake_label)\n",
        "    fake = torch.unsqueeze(fake,2)\n",
        "    fake = torch.cat((fake,conditional),2)\n",
        "    output = netD(fake.detach()).view(-1)\n",
        "\n",
        "    errD_fake = criterion(output, label)\n",
        "    errD_fake.backward()\n",
        "    D_G_z1 = output.mean().item()\n",
        "    errD = errD_real + errD_fake\n",
        "    optimizerD.step()\n",
        "\n",
        "\n",
        "    ############################\n",
        "    # (2) Update G network: maximize log(D(G(z)))\n",
        "    ###########################\n",
        "    netG.zero_grad()\n",
        "    label.fill_(real_label)  # fake labels are real for generator cost\n",
        "    # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "    output = netD(fake).view(-1)\n",
        "    errG = criterion(output, label)\n",
        "    errG.backward()\n",
        "    D_G_z2 = output.mean().item()\n",
        "\n",
        "\n",
        "\n",
        "    #additional condition\n",
        "    noise = torch.randn(batch_size, seq_len, 1, device=device)\n",
        "\n",
        "\n",
        "\n",
        "    # Update G\n",
        "    optimizerG.step()\n",
        "\n",
        "  if i % 1 == 0:\n",
        "    print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "    % (epoch, num_epochs, i, len(dataloader),\n",
        "       errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-57d8f6923dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Update G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lje4COw6v2Ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}